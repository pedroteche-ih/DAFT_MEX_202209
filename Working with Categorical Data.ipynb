{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_housing = pd.read_csv(\"data/tb_ames_housing.csv\")\n",
    "tb_housing = tb_housing.dropna(axis=1, thresh=tb_housing.shape[0] * 0.5)\n",
    "\n",
    "X_full = tb_housing.drop(\"SalePrice\", axis=1)\n",
    "y_full = tb_housing[\"SalePrice\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full)\n",
    "tb_housing_cat = X_train.select_dtypes(exclude=\"number\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treatment of Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with rare & missing levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_list = []\n",
    "mcl_list = []\n",
    "lcl_list = []\n",
    "na_list = []\n",
    "column_list = []\n",
    "for column in tb_housing_cat.columns:\n",
    "    num_levels = len(tb_housing_cat[column].unique())\n",
    "    most_common_level = tb_housing_cat[column].value_counts().max()\n",
    "    least_common_level = tb_housing_cat[column].value_counts().min()\n",
    "    num_non_na = (~tb_housing_cat[column].isna()).sum()\n",
    "    column_list.append(column)\n",
    "    nl_list.append(num_levels)\n",
    "    mcl_list.append(most_common_level)\n",
    "    lcl_list.append(least_common_level)\n",
    "    na_list.append(num_non_na)\n",
    "\n",
    "tb_cat_info = pd.DataFrame(\n",
    "    {\n",
    "        \"column\": column_list,\n",
    "        \"num_levels\": nl_list,\n",
    "        \"most_common_level\": mcl_list,\n",
    "        \"least_common_level\": lcl_list,\n",
    "        \"num_nonna\": na_list,\n",
    "    }\n",
    ")\n",
    "tb_cat_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_level_thresh = (tb_cat_info[\"most_common_level\"] / tb_cat_info[\"num_nonna\"]) < 0.9\n",
    "tb_cat_info = tb_cat_info[big_level_thresh].copy()\n",
    "tb_cat_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_cat_levels = pd.DataFrame()\n",
    "for column in tb_housing_cat.columns:\n",
    "    num_obs_level = tb_housing_cat[column].value_counts().reset_index()\n",
    "    num_obs_level.columns = [\"level_name\", \"num_obs\"]\n",
    "    num_obs_level[\"column\"] = column\n",
    "    tb_cat_levels = pd.concat([tb_cat_levels, num_obs_level], axis=0)\n",
    "\n",
    "tb_cat_levels.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_cat_level_info = tb_cat_info.merge(tb_cat_levels, on=\"column\")\n",
    "tb_cat_level_info.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_cat_level_info.loc[\n",
    "    tb_cat_level_info[\"num_obs\"] > 50, \"grp_level\"\n",
    "] = tb_cat_level_info[\"level_name\"]\n",
    "tb_cat_level_info.loc[tb_cat_level_info[\"num_obs\"] <= 50, \"grp_level\"] = \"Others\"\n",
    "tb_cat_level_info.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_cat_level_info.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_cat_level_info.groupby([\"column\", \"grp_level\"])[\"level_name\"].count().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in tb_cat_level_info[\"column\"].unique():\n",
    "    tb_column = tb_cat_level_info[tb_cat_level_info[\"column\"] == column]\n",
    "    grp_dict = dict()\n",
    "\n",
    "    for level in tb_column[\"level_name\"].unique():\n",
    "        grp_dict[level] = tb_column.loc[\n",
    "            tb_column[\"level_name\"] == level, \"grp_level\"\n",
    "        ].item()\n",
    "\n",
    "    new_column = \"grp_\" + column\n",
    "    X_train[new_column] = X_train[column].map(grp_dict).fillna(\"Others\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_group_dict(categorical_data, mcl_level=0.9, min_obs=50):\n",
    "    # Creating measurements of categorical data quality\n",
    "    nl_list = []\n",
    "    mcl_list = []\n",
    "    lcl_list = []\n",
    "    na_list = []\n",
    "    column_list = []\n",
    "    for column in categorical_data.columns:\n",
    "        num_levels = len(categorical_data[column].unique())\n",
    "        most_common_level = categorical_data[column].value_counts().max()\n",
    "        least_common_level = categorical_data[column].value_counts().min()\n",
    "        num_non_na = (~categorical_data[column].isna()).sum()\n",
    "        column_list.append(column)\n",
    "        nl_list.append(num_levels)\n",
    "        mcl_list.append(most_common_level)\n",
    "        lcl_list.append(least_common_level)\n",
    "        na_list.append(num_non_na)\n",
    "    tb_cat_info = pd.DataFrame(\n",
    "        {\n",
    "            \"column\": column_list,\n",
    "            \"num_levels\": nl_list,\n",
    "            \"most_common_level\": mcl_list,\n",
    "            \"least_common_level\": lcl_list,\n",
    "            \"num_nonna\": na_list,\n",
    "        }\n",
    "    )\n",
    "    # Filtering columns with low variance (most values are the same)\n",
    "    big_level_thresh = (\n",
    "        tb_cat_info[\"most_common_level\"] / tb_cat_info[\"num_nonna\"]\n",
    "    ) < mcl_level\n",
    "    tb_cat_info = tb_cat_info[big_level_thresh].copy()\n",
    "\n",
    "    # Creating categorical level information DataFrame\n",
    "    tb_cat_levels = pd.DataFrame()\n",
    "    for column in categorical_data.columns:\n",
    "        num_obs_level = categorical_data[column].value_counts().reset_index()\n",
    "        num_obs_level.columns = [\"level_name\", \"num_obs\"]\n",
    "        num_obs_level[\"column\"] = column\n",
    "        tb_cat_levels = pd.concat([tb_cat_levels, num_obs_level], axis=0)\n",
    "    tb_cat_level_info = tb_cat_info.merge(tb_cat_levels, on=\"column\")\n",
    "\n",
    "    # Creating grouping dictionaries for each categorical observation\n",
    "    tb_cat_level_info.loc[\n",
    "        tb_cat_level_info[\"num_obs\"] > min_obs, \"grp_level\"\n",
    "    ] = tb_cat_level_info[\"level_name\"]\n",
    "    tb_cat_level_info.loc[\n",
    "        tb_cat_level_info[\"num_obs\"] <= min_obs, \"grp_level\"\n",
    "    ] = \"Others\"\n",
    "    column_grp_dict = dict()\n",
    "    for column in tb_cat_level_info[\"column\"].unique():\n",
    "        tb_column = tb_cat_level_info[tb_cat_level_info[\"column\"] == column]\n",
    "        grp_dict = dict()\n",
    "\n",
    "        for level in tb_column[\"level_name\"].unique():\n",
    "            grp_dict[level] = tb_column.loc[\n",
    "                tb_column[\"level_name\"] == level, \"grp_level\"\n",
    "            ].item()\n",
    "\n",
    "        column_grp_dict[column] = grp_dict\n",
    "\n",
    "    return column_grp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log(y) = B + A * x\n",
    "# y = e^(B + A * x) = e^B * e^(A*x) = C * e^(A*x) * e^(A1 * x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_grp_dict = create_group_dict(X_train.select_dtypes(exclude=\"number\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_group_dict` is the `fit` part of our transformation. Now we must create a wrapper for the application of the dictionary (the `transform` part):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_cat_levels(categorical_data, col_grp_dict):\n",
    "    # Drop columns that are not mapped in our col_grp_dict\n",
    "    categorical_data = categorical_data[col_grp_dict.keys()].copy()\n",
    "    # Apply our dictionaries to the remaining columns\n",
    "    for column in col_grp_dict.keys():\n",
    "        grp_dict = col_grp_dict[column]\n",
    "        categorical_data[column] = (\n",
    "            categorical_data[column].map(grp_dict).fillna(\"Others\")\n",
    "        )\n",
    "\n",
    "    return categorical_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our wrappers on the housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_housing = pd.read_csv(\"data/tb_ames_housing.csv\")\n",
    "tb_housing = tb_housing.dropna(axis=1, thresh=tb_housing.shape[0] * 0.5)\n",
    "\n",
    "X_full = tb_housing.drop(\"SalePrice\", axis=1)\n",
    "y_full = tb_housing[\"SalePrice\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full)\n",
    "tb_housing_cat = X_train.select_dtypes(exclude=\"number\")\n",
    "\n",
    "col_grp_dict = create_group_dict(X_train.select_dtypes(exclude=\"number\"))\n",
    "X_train_cat_grp = group_cat_levels(\n",
    "    X_train.select_dtypes(exclude=\"number\"), col_grp_dict\n",
    ")\n",
    "X_test_cat_grp = group_cat_levels(X_test.select_dtypes(exclude=\"number\"), col_grp_dict)\n",
    "X_train_cat_grp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our One-Hot Encoder on our transformed categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_fit = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "ohe_fit.fit(X_train_cat_grp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dummy = pd.DataFrame(\n",
    "    ohe_fit.transform(X_train_cat_grp), columns=ohe_fit.get_feature_names_out()\n",
    ")\n",
    "X_test_dummy = pd.DataFrame(\n",
    "    ohe_fit.transform(X_test_cat_grp), columns=ohe_fit.get_feature_names_out()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Encoding Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing categorical levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "# You can also use mutual_info_classif for classification problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mir_fit = mutual_info_regression(X_train_dummy, y_train, discrete_features=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(mir_fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dummy.columns[mir_fit > np.quantile(mir_fit, 0.75)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca_fit = prince.MCA(n_components=10)\n",
    "mca_fit.fit(X_train_cat_grp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(mca_fit.explained_inertia_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca_fit.plot_coordinates(X_train_cat_grp, show_row_points=False, figsize=(10, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_mca = mca_fit.transform(X_train_cat_grp)\n",
    "tb_mca.columns = ['MC_' + str(i) for i in range(10)]\n",
    "tb_mca['log_SalePrice'] = np.log(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (10, 10))\n",
    "sns.scatterplot(data = tb_mca, x = 'MC_0', y = 'MC_2', hue = 'log_SalePrice', palette=\"Spectral\", alpha = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mca = mca_fit.transform(X_train_cat_grp)\n",
    "X_test_mca = mca_fit.transform(X_test_cat_grp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train.select_dtypes(include = 'number').fillna(0)\n",
    "X_test_num = X_test.select_dtypes(include = 'number').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_fit = PCA()\n",
    "pca_fit.fit(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pd.DataFrame(\n",
    "    pca_fit.transform(X_train_num),\n",
    "    columns = ['PC_' + str(i) for i in range(pca_fit.n_components_)],\n",
    "    index = y_train.index\n",
    ")\n",
    "X_test_pca = pd.DataFrame(\n",
    "    pca_fit.transform(X_test_num),\n",
    "    columns = ['PC_' + str(i) for i in range(pca_fit.n_components_)],\n",
    "    index = y_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = pd.concat([X_train_mca, X_train_pca], axis = 1)\n",
    "X_test_full = pd.concat([X_test_mca, X_test_pca], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_fit = CatBoostRegressor(iterations=20000, depth=8, od_type=\"Iter\", od_wait=1500, verbose = False)\n",
    "cat_fit.fit(X_train_full, y_train, eval_set=(X_test_full, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cat_fit.predict(X_test_full)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "014f4a4a5af8f0104b12c029e500f4146d6d785e8cf714d2a35b7a9514230cd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
