{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Transformers\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/pedroteche-ih/DAFT_MEX_202209/main/data/tb_hotel_traintest.csv\"\n",
    "tb_hotel = pd.read_csv(url, parse_dates=[\"arrival_date\", \"reservation_status_date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_hotel.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building & Evaluating Models in SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hotel_data(hotel_data):\n",
    "    hotel_data[\"children\"] = hotel_data[\"children\"].fillna(0)\n",
    "    hotel_data[\"country\"] = hotel_data[\"country\"].fillna(\"Unknown\")\n",
    "    hotel_data[\"is_company\"] = np.where(hotel_data[\"company\"].isna(), 0, 1)\n",
    "    hotel_data[\"is_agent\"] = np.where(hotel_data[\"agent\"].isna(), 0, 1)\n",
    "    hotel_data = hotel_data.drop(\n",
    "        [\"company\", \"agent\", \"id_booking\", \"reservation_status_date\"], axis=1\n",
    "    ).dropna()\n",
    "\n",
    "    return hotel_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_hotel_clean = clean_hotel_data(tb_hotel)\n",
    "\n",
    "cat_vars = list(tb_hotel_clean.select_dtypes(\"object\").columns)\n",
    "num_vars = list(tb_hotel_clean.select_dtypes(include=np.number).drop(\"is_cancelled\", axis=1).columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tb_hotel_clean.drop(\"is_cancelled\", axis=1)\n",
    "y = tb_hotel_clean[\"is_cancelled\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train.select_dtypes(include=\"number\")\n",
    "X_test_num = X_test[X_train_num.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_trans = PowerTransformer()\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "dt_fit = DecisionTreeClassifier()\n",
    "log_fit = LogisticRegression()\n",
    "\n",
    "X_train_num_t = power_trans.fit_transform(X_train_num)\n",
    "X_train_num_s = scaler.fit_transform(X_train_num_t)\n",
    "X_train_num_p = scaler.fit_transform(X_train_num_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fit.fit(X_train_num_p, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_grid = {\n",
    "    \"max_depth\": [int(x) for x in np.linspace(1, 150, 5)],\n",
    "    \"min_samples_split\": [int(x) for x in np.linspace(2, 100, 5)],\n",
    "    \"min_samples_leaf\": [int(x) for x in np.linspace(1, 50, 5)],\n",
    "}\n",
    "\n",
    "cv_fit = GridSearchCV(dt_fit, param_grid=para_grid)\n",
    "cv_fit.fit(X_train_num_p, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num_t = power_trans.transform(X_test_num)\n",
    "X_test_num_s = scaler.transform(X_test_num_t)\n",
    "X_test_num_p = scaler.transform(X_test_num_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_log = log_fit.predict(X_test_num_p)\n",
    "y_pred_dt = cv_fit.predict(X_test_num_p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "\n",
    "*Out of all predicitions, what % were correct?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_acc = np.round(accuracy_score(y_test, y_pred_log), 2)\n",
    "dt_acc = np.round(accuracy_score(y_test, y_pred_dt), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Logistic Regression Accuracy: {log_acc}\")\n",
    "print(f\"D.T. Regression Accuracy: {dt_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "*Out of all cancellation predicitions, what % were correct?*\n",
    "\n",
    "**Precision is the ratio between True Positives (correct positive predictions) and True+False Positives (all positive predictions).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prc = np.round(precision_score(y_test, y_pred_log), 2)\n",
    "dt_prc = np.round(precision_score(y_test, y_pred_dt), 2)\n",
    "\n",
    "print(f\"Logistic Regression Precision: {log_prc}\")\n",
    "print(f\"D.T. Regression Precision: {dt_prc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "\n",
    "*Out of all real cancellations, what % were correctly predicted?*\n",
    "\n",
    "**Recall is the ration between True Positives (correct positive predictions) and Real Positives (observed positive outcomes).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rec = np.round(recall_score(y_test, y_pred_log), 2)\n",
    "dt_rec = np.round(recall_score(y_test, y_pred_dt), 2)\n",
    "\n",
    "print(f\"Logistic Regression Recall: {log_rec}\")\n",
    "print(f\"D.T. Regression Recall: {dt_rec}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1-Score\n",
    "\n",
    "**The *harmonic mean* between precision and recall:**\n",
    "\n",
    "$$f_1 = 2 \\frac{precision * recall}{precision + recall}$$\n",
    "\n",
    "A **f1-score = 1** means our model has perfect precision (all its cancellation predictions were cancellations) and perfect recall (all observed cancellations were predicted correctly). The lower the f1-score the farther we are from a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_f1 = np.round(f1_score(y_test, y_pred_log), 2)\n",
    "dt_f1 = np.round(f1_score(y_test, y_pred_dt), 2)\n",
    "\n",
    "print(f\"Logistic Regression F1: {log_f1}\")\n",
    "print(f\"D.T. Regression F1: {dt_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilizing precision, recall and F1 for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    prc = np.round(precision_score(y_true, y_pred), 2)\n",
    "    f1 = np.round(f1_score(y_true, y_pred), 2)\n",
    "    rec = np.round(recall_score(y_true, y_pred), 2)\n",
    "\n",
    "    print(f\"Logistic Regression F1: {f1}\")\n",
    "    print(f\"Logistic Regression Precision: {prc}\")\n",
    "    print(f\"Logistic Regression Recall: {rec}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(y_test, y_pred_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(y_test, y_pred_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_trans = PowerTransformer()\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "dt_fit = DecisionTreeClassifier()\n",
    "blocks = [(\"TRANS\", power_trans), (\"SCALE\", scaler), (\"PCA\", pca), (\"MODEL\", dt_fit)]\n",
    "\n",
    "pipeline = Pipeline(blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_grid = {\n",
    "    \"PCA__n_components\": range(1, len(num_vars)),\n",
    "    \"MODEL__max_depth\": [int(x) for x in np.linspace(1, 150, 50)],\n",
    "    \"MODEL__min_samples_split\": [int(x) for x in np.linspace(2, 100, 50)],\n",
    "    \"MODEL__min_samples_leaf\": [int(x) for x in np.linspace(1, 50, 50)],\n",
    "}\n",
    "\n",
    "cv_fit = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=para_grid, n_iter=10, scoring=\"f1\"\n",
    ")\n",
    "cv_fit.fit(X_train_num, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pipeline = cv_fit.predict(X_test_num)\n",
    "evaluate_model(y_test, y_pred_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(y_test, y_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_fit.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Complex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_trans = PowerTransformer()\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "num_blocks = [(\"TRANS\", power_trans), (\"SCALE\", scaler), (\"PCA\", pca)]\n",
    "\n",
    "num_pipeline = Pipeline(num_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\")\n",
    "kbest = SelectKBest(score_func=mutual_info_classif)\n",
    "cat_blocks = [(\"OHE\", ohe), (\"KB\", kbest)]\n",
    "\n",
    "cat_pipeline = Pipeline(cat_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"NUMPREP\", num_pipeline, num_vars),\n",
    "        (\"CATPREP\", cat_pipeline, cat_vars),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fit = LogisticRegression()\n",
    "pipeline = Pipeline(steps=[('PRE', data_prep_pipeline),\n",
    "                           ('MODEL', log_fit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'PRE__NUMPREP__PCA__n_components' : range(1, len(num_vars)),\n",
    "    'PRE__CATPREP__KB__k' : range(1, len(cat_vars))\n",
    "}\n",
    "\n",
    "grid_fit = RandomizedSearchCV(pipeline, param_grid, n_iter = 10, n_jobs = 7)\n",
    "grid_fit.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pipeline = grid_fit.predict(X_test)\n",
    "evaluate_model(y_test, y_pred_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_wine = pd.DataFrame(datasets.load_wine(as_frame=True)['data'])\n",
    "tb_wine['classif_wine'] = pd.DataFrame(datasets.load_wine(as_frame=True)['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tb_wine.drop('classif_wine', axis = 1)\n",
    "y = tb_wine['classif_wine']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_trans = PowerTransformer()\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "dt_fit = DecisionTreeClassifier()\n",
    "blocks = [(\"TRANS\", power_trans), (\"SCALE\", scaler), (\"PCA\", pca), (\"MODEL\", dt_fit)]\n",
    "\n",
    "pipeline = Pipeline(blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_grid = {\n",
    "    \"PCA__n_components\": range(1, X.shape[1]),\n",
    "    \"MODEL__max_depth\": [int(x) for x in np.linspace(1, 150, 50)],\n",
    "    \"MODEL__min_samples_split\": [int(x) for x in np.linspace(2, 100, 50)],\n",
    "    \"MODEL__min_samples_leaf\": [int(x) for x in np.linspace(1, 50, 50)],\n",
    "}\n",
    "\n",
    "cv_fit = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=para_grid, n_iter=1000, scoring=\"f1\"\n",
    ")\n",
    "cv_fit.fit(X_train, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cv_fit.predict(X_test)\n",
    "f1_score(y_test, y_pred, average = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model w/ Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_trans = PowerTransformer()\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "dt_fit = DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "blocks = [(\"TRANS\", power_trans), (\"SCALE\", scaler), (\"PCA\", pca), (\"MODEL\", dt_fit)]\n",
    "\n",
    "pipeline = Pipeline(blocks)\n",
    "\n",
    "para_grid = {\n",
    "    \"PCA__n_components\": range(1, X.shape[1]),\n",
    "    \"MODEL__max_depth\": [int(x) for x in np.linspace(1, 150, 50)],\n",
    "    \"MODEL__min_samples_split\": [int(x) for x in np.linspace(2, 100, 50)],\n",
    "    \"MODEL__min_samples_leaf\": [int(x) for x in np.linspace(1, 50, 50)],\n",
    "}\n",
    "\n",
    "cv_fit = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=para_grid, n_iter=1000, scoring=\"f1\"\n",
    ")\n",
    "cv_fit.fit(X_train, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cv_fit.predict(X_test)\n",
    "f1_score(y_test, y_pred, average = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred, average = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the aggregate error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Micro averaging\n",
    "\n",
    "Calculates True Positives, False Positives and False Negatives for each class, totalling them and calculating the F1 score for these totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average = 'micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macro averaging\n",
    "\n",
    "**Unweighted average** F1-Score for each class - **gives equal importance to different classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted averaging\n",
    "\n",
    "**Weighted average** F1-Score for each class - **gives proportional weights to classes with more observations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average = 'weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "014f4a4a5af8f0104b12c029e500f4146d6d785e8cf714d2a35b7a9514230cd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
