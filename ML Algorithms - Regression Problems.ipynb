{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building from yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/pedroteche-ih/DAFT_MEX_202209/main/data/tb_ames_housing.csv\"\n",
    "tb_housing = pd.read_csv(url)\n",
    "tb_housing.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_housing.isna().sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill FireplaceQu for houses with no fireplace\n",
    "tb_housing[\"FireplaceQu\"] = tb_housing[\"FireplaceQu\"].fillna(\"NF\")\n",
    "# Dropping columns with over 50% missing values\n",
    "tb_housing = tb_housing.dropna(axis=1, thresh=tb_housing.shape[0] * 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_trans(data):\n",
    "    data[\"log_SalePrice\"] = np.log(data[\"SalePrice\"])\n",
    "    data[\"log_GrLivArea\"] = np.log(data[\"GrLivArea\"])\n",
    "    data[\"lot_ocuppation\"] = data[\"GrLivArea\"] + data[\"LotArea\"]\n",
    "    heatingqc_group = {\n",
    "        \"Ex\": \"Good\",\n",
    "        \"Gd\": \"Good\",\n",
    "        \"TA\": \"Bad\",\n",
    "        \"Fa\": \"Bad\",\n",
    "        \"Po\": \"Bad\",\n",
    "    }\n",
    "    data[\"grp_HeatingQC\"] = data[\"HeatingQC\"].map(heatingqc_group)\n",
    "    mszoning_group = {\"RL\": \"RL\", \"RM\": \"RM\"}\n",
    "    data[\"grp_MSZoning\"] = data[\"MSZoning\"].map(mszoning_group)\n",
    "    data[\"grp_MSZoning\"] = data[\"grp_MSZoning\"].fillna(\"Other\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_housing_trans = var_trans(tb_housing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_fit(train_data):\n",
    "    # Fitting Scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data[[\"log_GrLivArea\", \"OverallQual\", \"lot_ocuppation\"]])\n",
    "\n",
    "    # Fitting OneHotEncoder\n",
    "    ohe = OneHotEncoder(drop=\"first\", sparse=False, handle_unknown=\"ignore\")\n",
    "    ohe.fit(train_data[[\"grp_HeatingQC\", \"grp_MSZoning\"]])\n",
    "\n",
    "    return scaler, ohe\n",
    "\n",
    "\n",
    "def transformer_apply(train_data, scaler, ohe):\n",
    "    # Applying scaler\n",
    "    feature_names = [\"sca_\" + name for name in scaler.feature_names_in_]\n",
    "    scaled_matrix = scaler.transform(\n",
    "        train_data[[\"log_GrLivArea\", \"OverallQual\", \"lot_ocuppation\"]]\n",
    "    )\n",
    "    scaled_data = pd.DataFrame(scaled_matrix, columns=feature_names)\n",
    "\n",
    "    # Applying OHE\n",
    "    dummy_names = ohe.get_feature_names_out()\n",
    "    ohe_matrix = ohe.transform(train_data[[\"grp_HeatingQC\", \"grp_MSZoning\"]])\n",
    "    dummy_data = pd.DataFrame(ohe_matrix, columns=dummy_names)\n",
    "\n",
    "    return pd.concat([scaled_data, dummy_data], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_housing_trans = var_trans(tb_housing)\n",
    "x_var = [\n",
    "    \"log_GrLivArea\",\n",
    "    \"OverallQual\",\n",
    "    \"lot_ocuppation\",\n",
    "    \"grp_HeatingQC\",\n",
    "    \"grp_MSZoning\",\n",
    "]\n",
    "y_var = \"log_SalePrice\"\n",
    "model_var = x_var + [y_var]\n",
    "tb_housing_model = tb_housing_trans[model_var].dropna()\n",
    "\n",
    "X = tb_housing_model[x_var]\n",
    "y = tb_housing_model[y_var]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scaler, ohe = transformer_fit(X_train)\n",
    "X_train_trans = transformer_apply(X_train, scaler, ohe)\n",
    "X_test_trans = transformer_apply(X_test, scaler, ohe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Techniques - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_fit = LinearRegression()\n",
    "lm_fit.fit(X_train_trans, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm_fit.predict(X_test_trans)\n",
    "tb_test = pd.DataFrame({\"y_real\": y_test, \"y_pred\": y_pred})\n",
    "tb_test = pd.concat([tb_test, X_test], axis=1)\n",
    "tb_test[\"SalePrice\"] = np.exp(tb_test[\"y_real\"])\n",
    "tb_test[\"pred_lm_SalePrice\"] = np.exp(tb_test[\"y_pred\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(tb_test[\"SalePrice\"], tb_test[\"pred_lm_SalePrice\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN\n",
    "\n",
    "**Hyper-parameters:**\n",
    "\n",
    "1. `n_neighbors` - number of k nearest neighbors used to build model.\n",
    "1. `weights` - `\"uniform\"` or `\"distance\"`, how neighbor values are weighed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fit = KNeighborsRegressor(n_neighbors=1)\n",
    "knn_fit.fit(X_train_trans, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_fit.predict(X_test_trans)\n",
    "tb_test[\"pred_1nn_SalePrice\"] = np.exp(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(\n",
    "    np.sqrt(mean_squared_error(tb_test[\"SalePrice\"], tb_test[\"pred_1nn_SalePrice\"])), 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_k = [1, 5, 10, 25, 50]\n",
    "for k in n_k:\n",
    "    knn_fit = KNeighborsRegressor(n_neighbors=k, weights=\"distance\")\n",
    "    knn_fit.fit(X_train_trans, y_train)\n",
    "    y_pred = knn_fit.predict(X_test_trans)\n",
    "    error = np.round(\n",
    "        np.sqrt(mean_squared_error(tb_test[\"SalePrice\"], np.exp(y_pred))), 2\n",
    "    )\n",
    "    print(f\"{k}-NN RMSE: {error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "**Hyperparameters:**\n",
    "1. `max_depth`: maximum depth of tree\n",
    "1. `min_samples_leaf`: minimum number of training points at each leaf\n",
    "1. `min_samples_split`: minimum number of training points at each branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_fit = DecisionTreeRegressor()\n",
    "dt_fit.fit(X_train_trans, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt_fit.predict(X_test_trans)\n",
    "tb_test[\"pred_dt_SalePrice\"] = np.exp(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(\n",
    "    np.sqrt(mean_squared_error(tb_test[\"SalePrice\"], tb_test[\"pred_dt_SalePrice\"])), 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_fit.get_depth()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = dt_fit.predict(X_train_trans)\n",
    "np.sqrt(mean_squared_error(np.exp(y_train), np.exp(y_train_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Overfit Decision Trees\n",
    "\n",
    "A decision tree's complexity is directly proportional to the amount of overfitting: more complex trees will have a higher probability of overfitting compared to simples trees.\n",
    "\n",
    "Let's limit our tree's complexity using our 3 hyperparameters:\n",
    "\n",
    "1. The deeper a tree (**larger `max_depth`**), the **more complex** the tree;\n",
    "1. The smaller the leafs (**smaller `min_samples_leaf`**), the **more complex** the tree;\n",
    "1. The smaller the branches (**smaller `min_samples_split`**), the **more complex** the tree;\n",
    "\n",
    "The easiest way to reduce a tree's complexity is by training trees of smaller `max_depth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_fit = DecisionTreeRegressor(max_depth=5)\n",
    "dt_fit.fit(X_train_trans, y_train)\n",
    "y_pred = dt_fit.predict(X_test_trans)\n",
    "tb_test[\"pred_dt_SalePrice\"] = np.exp(y_pred)\n",
    "np.round(\n",
    "    np.sqrt(mean_squared_error(tb_test[\"SalePrice\"], tb_test[\"pred_dt_SalePrice\"])), 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **nested loops** to test different hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_list = range(10, 201, 5)\n",
    "min_samples_leaf_list = range(1, 50, 5)\n",
    "min_samples_split_list = range(2, 100, 5)\n",
    "errors = []\n",
    "for depth in max_depth_list:\n",
    "    for leaf in min_samples_leaf_list:\n",
    "        for split in min_samples_split_list:\n",
    "            dt_fit = DecisionTreeRegressor(\n",
    "                max_depth=depth, min_samples_leaf=leaf, min_samples_split=split\n",
    "            )\n",
    "            dt_fit.fit(X_train_trans, y_train)\n",
    "            y_pred = dt_fit.predict(X_test_trans)\n",
    "            tb_test[\"pred_dt_SalePrice\"] = np.exp(y_pred)\n",
    "            error = np.round(\n",
    "                np.sqrt(\n",
    "                    mean_squared_error(\n",
    "                        tb_test[\"SalePrice\"], tb_test[\"pred_dt_SalePrice\"]\n",
    "                    )\n",
    "                ),\n",
    "                2,\n",
    "            )\n",
    "            errors.append((depth, leaf, split, error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(errors, columns=[\"depth\", \"leaf\", \"split\", \"error\"]).sort_values(\"error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_fit = DecisionTreeRegressor(max_depth= 6, min_samples_leaf = 13, min_samples_split = 28)\n",
    "dt_fit.fit(X_train_trans, y_train)\n",
    "plt.figure(figsize = (35, 10))\n",
    "plot_tree(dt_fit, feature_names=X_train_trans.columns, max_depth = 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(dt_fit, 'teste.dot', feature_names=X_train_trans.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "Alongside deep-learning methods, ensemble algorithms are currently the best tools available for predictive modelling. They have the precision associated with decision trees, while adopting several strategies to mitigate the overfitting risk.\n",
    "\n",
    "Let's see the two main methods of ensemble regressors: bagging and boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "**RandomForestRegressor Hyperparameters**:\n",
    "\n",
    "1. `n_estimators`: number of weak learners (the more weak learners, the more complex the model);\n",
    "1. `max_depth`: complexity of each weak learner (the more complex each weak learner, the more complex the model); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fit = RandomForestRegressor(n_estimators=1000, max_depth=1)  # RandomStumps\n",
    "rf_fit.fit(X_train_trans, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt_fit.predict(X_test_trans)\n",
    "tb_test[\"pred_rf_SalePrice\"] = np.exp(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(\n",
    "    np.sqrt(mean_squared_error(tb_test[\"SalePrice\"], tb_test[\"pred_rf_SalePrice\"])), 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing w/ GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = {\"n_estimators\": range(10, 1001, 50), \"max_depth\": range(1, 20, 2)}\n",
    "rf_fit = RandomForestRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_rf_fit = GridSearchCV(rf_fit, hyper_parameters, n_jobs=-1)\n",
    "cv_rf_fit.fit(X_train_trans, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_rf_fit.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cv_rf_fit.predict(X_test_trans)\n",
    "tb_test[\"pred_rf_SalePrice\"] = np.exp(y_pred)\n",
    "np.round(\n",
    "    np.sqrt(mean_squared_error(tb_test[\"SalePrice\"], tb_test[\"pred_rf_SalePrice\"])), 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cat Boosting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_fit = cat.CatBoostRegressor(iterations=20000, depth=3, od_type=\"Iter\", od_wait=500)\n",
    "cat_fit.fit(X_train_trans, y_train, eval_set=(X_test_trans, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cat_fit.predict(X_test_trans)\n",
    "tb_test[\"pred_cat_SalePrice\"] = np.exp(y_pred)\n",
    "np.round(\n",
    "    np.sqrt(mean_squared_error(tb_test[\"SalePrice\"], tb_test[\"pred_cat_SalePrice\"])), 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including more Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_housing_trans.select_dtypes(include=\"number\").columns[\n",
    "    np.abs(tb_housing_trans.corr()[\"SalePrice\"]).sort_values() > 0.2\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_housing_trans = var_trans(tb_housing)\n",
    "x_var = [\n",
    "    \"log_GrLivArea\",\n",
    "    \"OverallQual\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"GrLivArea\",\n",
    "    \"BsmtFullBath\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"FullBath\",\n",
    "    \"HalfBath\",\n",
    "    \"BedroomAbvGr\",\n",
    "    \"KitchenAbvGr\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"Fireplaces\",\n",
    "    \"GarageYrBlt\",\n",
    "    \"GarageCars\",\n",
    "    \"GarageArea\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"OpenPorchSF\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\",\n",
    "    \"PoolArea\",\n",
    "    \"MiscVal\",\n",
    "    \"lot_ocuppation\",\n",
    "    \"grp_HeatingQC\",\n",
    "    \"grp_MSZoning\",\n",
    "]\n",
    "y_var = \"log_SalePrice\"\n",
    "model_var = x_var + [y_var]\n",
    "tb_housing_model = tb_housing_trans[model_var].dropna()\n",
    "\n",
    "X = tb_housing_model[x_var]\n",
    "y = tb_housing_model[y_var]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scaler, ohe = transformer_fit(X_train)\n",
    "X_train_trans = transformer_apply(X_train, scaler, ohe)\n",
    "X_test_trans = transformer_apply(X_test, scaler, ohe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_fit = LinearRegression()\n",
    "lm_fit.fit(X_train_trans, y_train)\n",
    "y_pred = lm_fit.predict(X_test_trans)\n",
    "np.sqrt(mean_squared_error(np.exp(y_test), np.exp(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_fit = cat.CatBoostRegressor(iterations=20000, depth=8, od_type=\"Iter\", od_wait=1500, verbose = False)\n",
    "cat_fit.fit(X_train_trans, y_train, eval_set=(X_test_trans, y_test))\n",
    "y_pred = cat_fit.predict(X_test_trans)\n",
    "np.sqrt(mean_squared_error(np.exp(y_test), np.exp(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "014f4a4a5af8f0104b12c029e500f4146d6d785e8cf714d2a35b7a9514230cd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
